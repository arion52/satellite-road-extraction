{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2024-02-03T09:18:23.860138Z","iopub.status.busy":"2024-02-03T09:18:23.859365Z","iopub.status.idle":"2024-02-03T09:18:28.860500Z","shell.execute_reply":"2024-02-03T09:18:28.859677Z","shell.execute_reply.started":"2024-02-03T09:18:23.860029Z"},"papermill":{"duration":4.206007,"end_time":"2021-09-25T04:59:24.659764","exception":false,"start_time":"2021-09-25T04:59:20.453757","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\n","# This script imports necessary libraries for image processing and deep learning using U-Net architecture.\n","import os\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T09:18:28.862772Z","iopub.status.busy":"2024-02-03T09:18:28.862223Z","iopub.status.idle":"2024-02-03T09:18:28.872885Z","shell.execute_reply":"2024-02-03T09:18:28.871815Z","shell.execute_reply.started":"2024-02-03T09:18:28.862742Z"},"papermill":{"duration":0.024113,"end_time":"2021-09-25T04:59:24.695402","exception":false,"start_time":"2021-09-25T04:59:24.671289","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["framObjTrain = {'img' : [],\n","           'mask' : []\n","          }\n","\n","def LoadData(frameObj=None, imgPath=None, maskPath=None, shape=128):\n","    \"\"\"\n","    Load data from image and mask directories and resize them to the specified shape.\n","\n","    Args:\n","        frameObj (dict): Dictionary to store the loaded images and masks.\n","        imgPath (str): Path to the directory containing the images.\n","        maskPath (str): Path to the directory containing the masks.\n","        shape (int): Desired shape for resizing the images and masks.\n","\n","    Returns:\n","        dict: Dictionary containing the loaded and resized images and masks.\n","    \"\"\"\n","    imgNames = os.listdir(imgPath)\n","    maskNames = []\n","\n","    ## generating mask names\n","    for mem in imgNames:\n","        mem = mem.split('_')[0]\n","        if mem not in maskNames:\n","            maskNames.append(mem)\n","\n","    imgAddr = imgPath + '/'\n","    maskAddr = maskPath + '/'\n","\n","    for i in range(len(imgNames)):\n","        try:\n","            img = plt.imread(imgAddr + maskNames[i] + '_sat.jpg')\n","            mask = plt.imread(maskAddr + maskNames[i] + '_mask.png')\n","\n","        except:\n","            continue\n","        img = cv2.resize(img, (shape, shape))\n","        mask = cv2.resize(mask, (shape, shape))\n","        frameObj['img'].append(img)\n","        frameObj['mask'].append(mask[:, :, 0])  # this is because its a binary mask and img is present in channel 0\n","\n","    return frameObj"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T09:18:28.874768Z","iopub.status.busy":"2024-02-03T09:18:28.874411Z","iopub.status.idle":"2024-02-03T09:18:28.899839Z","shell.execute_reply":"2024-02-03T09:18:28.898904Z","shell.execute_reply.started":"2024-02-03T09:18:28.874740Z"},"papermill":{"duration":0.040037,"end_time":"2021-09-25T04:59:24.745757","exception":false,"start_time":"2021-09-25T04:59:24.70572","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def Conv2dBlock(inputTensor, numFilters, kernelSize = 3, doBatchNorm = True):\n","    \"\"\"\n","    Creates a convolutional block consisting of two convolutional layers with batch normalization and ReLU activation.\n","    \n","    Parameters:\n","    inputTensor (Tensor): Input tensor to the convolutional block.\n","    numFilters (int): Number of filters for the convolutional layers.\n","    kernelSize (int): Size of the kernel for the convolutional layers. Default is 3.\n","    doBatchNorm (bool): Whether to apply batch normalization. Default is True.\n","    \n","    Returns:\n","    Tensor: Output tensor from the convolutional block.\n","    \"\"\"\n","    x = tf.keras.layers.Conv2D(filters = numFilters, kernel_size = (kernelSize, kernelSize),\n","                              kernel_initializer = 'he_normal', padding = 'same') (inputTensor)\n","    \n","    if doBatchNorm:\n","        x = tf.keras.layers.BatchNormalization()(x)\n","        \n","    x =tf.keras.layers.Activation('relu')(x)\n","    \n","    x = tf.keras.layers.Conv2D(filters = numFilters, kernel_size = (kernelSize, kernelSize),\n","                              kernel_initializer = 'he_normal', padding = 'same') (x)\n","    if doBatchNorm:\n","        x = tf.keras.layers.BatchNormalization()(x)\n","        \n","    x = tf.keras.layers.Activation('relu')(x)\n","    \n","    return x\n","\n","def GiveMeUnet(inputImage, numFilters = 16, droupouts = 0.1, doBatchNorm = True):\n","    \"\"\"\n","    Creates a U-Net model for semantic segmentation.\n","\n","    Parameters:\n","    - inputImage: Input image tensor.\n","    - numFilters: Number of filters in the first convolutional layer (default: 16).\n","    - droupouts: Dropout rate (default: 0.1).\n","    - doBatchNorm: Whether to apply batch normalization (default: True).\n","\n","    Returns:\n","    - model: U-Net model for semantic segmentation.\n","    \"\"\"\n","    c1 = Conv2dBlock(inputImage, numFilters * 1, kernelSize = 3, doBatchNorm = doBatchNorm)\n","    p1 = tf.keras.layers.MaxPooling2D((2,2))(c1)\n","    p1 = tf.keras.layers.Dropout(droupouts)(p1)\n","    \n","    c2 = Conv2dBlock(p1, numFilters * 2, kernelSize = 3, doBatchNorm = doBatchNorm)\n","    p2 = tf.keras.layers.MaxPooling2D((2,2))(c2)\n","    p2 = tf.keras.layers.Dropout(droupouts)(p2)\n","    \n","    c3 = Conv2dBlock(p2, numFilters * 4, kernelSize = 3, doBatchNorm = doBatchNorm)\n","    p3 = tf.keras.layers.MaxPooling2D((2,2))(c3)\n","    p3 = tf.keras.layers.Dropout(droupouts)(p3)\n","    \n","    c4 = Conv2dBlock(p3, numFilters * 8, kernelSize = 3, doBatchNorm = doBatchNorm)\n","    p4 = tf.keras.layers.MaxPooling2D((2,2))(c4)\n","    p4 = tf.keras.layers.Dropout(droupouts)(p4)\n","    \n","    c5 = Conv2dBlock(p4, numFilters * 16, kernelSize = 3, doBatchNorm = doBatchNorm)\n","    \n","    u6 = tf.keras.layers.Conv2DTranspose(numFilters*8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n","    u6 = tf.keras.layers.concatenate([u6, c4])\n","    u6 = tf.keras.layers.Dropout(droupouts)(u6)\n","    c6 = Conv2dBlock(u6, numFilters * 8, kernelSize = 3, doBatchNorm = doBatchNorm)\n","    \n","    u7 = tf.keras.layers.Conv2DTranspose(numFilters*4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n","    u7 = tf.keras.layers.concatenate([u7, c3])\n","    u7 = tf.keras.layers.Dropout(droupouts)(u7)\n","    c7 = Conv2dBlock(u7, numFilters * 4, kernelSize = 3, doBatchNorm = doBatchNorm)\n","    \n","    u8 = tf.keras.layers.Conv2DTranspose(numFilters*2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n","    u8 = tf.keras.layers.concatenate([u8, c2])\n","    u8 = tf.keras.layers.Dropout(droupouts)(u8)\n","    c8 = Conv2dBlock(u8, numFilters * 2, kernelSize = 3, doBatchNorm = doBatchNorm)\n","    \n","    u9 = tf.keras.layers.Conv2DTranspose(numFilters*1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n","    u9 = tf.keras.layers.concatenate([u9, c1])\n","    u9 = tf.keras.layers.Dropout(droupouts)(u9)\n","    c9 = Conv2dBlock(u9, numFilters * 1, kernelSize = 3, doBatchNorm = doBatchNorm)\n","    \n","    output = tf.keras.layers.Conv2D(1, (1, 1), activation = 'sigmoid')(c9)\n","    model = tf.keras.Model(inputs = [inputImage], outputs = [output])\n","    return model\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T09:18:28.903479Z","iopub.status.busy":"2024-02-03T09:18:28.902682Z","iopub.status.idle":"2024-02-03T09:18:33.836750Z","shell.execute_reply":"2024-02-03T09:18:33.835860Z","shell.execute_reply.started":"2024-02-03T09:18:28.903442Z"},"papermill":{"duration":3.002494,"end_time":"2021-09-25T04:59:27.758345","exception":false,"start_time":"2021-09-25T04:59:24.755851","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\"\"\"\n","This code block initializes the input layer for the U-Net model with a shape of (128, 128, 3).\n","It then creates a U-Net model using the `GiveMeUnet` function, with a dropout rate of 0.07.\n","The model is compiled with the Adam optimizer, binary crossentropy loss, and accuracy metric.\n","\"\"\"\n","inputs = tf.keras.layers.Input((128, 128, 3))\n","unet = GiveMeUnet(inputs, droupouts= 0.07)\n","unet.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'] )\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T09:18:33.838198Z","iopub.status.busy":"2024-02-03T09:18:33.837890Z","iopub.status.idle":"2024-02-03T09:25:14.581939Z","shell.execute_reply":"2024-02-03T09:25:14.581056Z","shell.execute_reply.started":"2024-02-03T09:18:33.838170Z"},"papermill":{"duration":378.836926,"end_time":"2021-09-25T05:05:47.906782","exception":false,"start_time":"2021-09-25T04:59:29.069856","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\"\"\"\n","This function is used to train the U-Net model for object detection.\n","\"\"\"\n","trainPath = '/kaggle/input/deepglobe-road-extraction-dataset/train'\n","framObjTrain = LoadData( framObjTrain, imgPath = trainPath, \n","                        maskPath = trainPath\n","                         , shape = 128)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T09:25:14.583849Z","iopub.status.busy":"2024-02-03T09:25:14.583445Z","iopub.status.idle":"2024-02-03T09:25:14.829060Z","shell.execute_reply":"2024-02-03T09:25:14.828066Z","shell.execute_reply.started":"2024-02-03T09:25:14.583810Z"},"papermill":{"duration":0.287128,"end_time":"2021-09-25T05:05:48.216207","exception":false,"start_time":"2021-09-25T05:05:47.929079","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\"\"\"\n","Display the image and mask using subplots.\n","\n","This function plots the image and mask using subplots. It assumes that the `framObjTrain` variable\n","contains a dictionary with the 'img' and 'mask' keys, where the values are arrays representing\n","the image and mask respectively.\n","\n","\"\"\"\n","plt.subplot(1,2,1)\n","plt.imshow(framObjTrain['img'][1])\n","plt.subplot(1,2,2)\n","plt.imshow(framObjTrain['mask'][1])\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T09:25:14.830803Z","iopub.status.busy":"2024-02-03T09:25:14.830442Z","iopub.status.idle":"2024-02-03T09:25:14.838005Z","shell.execute_reply":"2024-02-03T09:25:14.837031Z","shell.execute_reply.started":"2024-02-03T09:25:14.830770Z"},"trusted":true},"outputs":[],"source":["len(framObjTrain['img'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T09:25:14.839708Z","iopub.status.busy":"2024-02-03T09:25:14.839319Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Train a U-Net model using the given image and mask data.\n","\"\"\"\n","retVal = unet.fit(np.array(framObjTrain['img']), np.array(framObjTrain['mask']), epochs = 83, verbose = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"\n","Plots the training loss and accuracy from the `retVal` history.\n","\n","Parameters:\n","- retVal (object): The history object containing the training loss and accuracy.\n","\n","Returns:\n","None\n","\"\"\"\n","plt.plot(retVal.history['loss'], label='training_loss')\n","plt.plot(retVal.history['accuracy'], label='training_accuracy')\n","plt.legend()\n","plt.grid(True)"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":2.469852,"end_time":"2021-09-25T05:57:34.721045","exception":false,"start_time":"2021-09-25T05:57:32.251193","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def predict16(valMap, model, shape=128):\n","    \"\"\"\n","    Predicts the output for a batch of 16 images using the given model.\n","\n","    Args:\n","        valMap (dict): A dictionary containing the input images and masks.\n","        model: The trained model used for prediction.\n","        shape (int): The shape of the images.\n","\n","    Returns:\n","        predictions: The predicted output for the batch of images.\n","        imgProc: The processed input images.\n","        mask: The ground truth masks.\n","    \"\"\"\n","    img = valMap['img'][0:16]\n","    mask = valMap['mask'][0:16]\n","    \n","    imgProc = img[0:16]\n","    imgProc = np.array(img)\n","    \n","    predictions = model.predict(imgProc)\n","\n","    return predictions, imgProc, mask\n","\n","\n","def Plotter(img, predMask, groundTruth):\n","    \"\"\"\n","    Plots the aerial image, predicted routes, and actual routes side by side.\n","\n","    Args:\n","        img (numpy.ndarray): The aerial image.\n","        predMask (numpy.ndarray): The predicted routes.\n","        groundTruth (numpy.ndarray): The actual routes.\n","    \"\"\"\n","    plt.figure(figsize=(9,9))\n","    \n","    plt.subplot(1,3,1)\n","    plt.imshow(img)\n","    plt.title('Aerial image')\n","    \n","    plt.subplot(1,3,2)\n","    plt.imshow(predMask)\n","    plt.title('Predicted Routes')\n","    \n","    plt.subplot(1,3,3)\n","    plt.imshow(groundTruth)\n","    plt.title('Actual Routes')"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":3.36507,"end_time":"2021-09-25T05:57:40.29918","exception":false,"start_time":"2021-09-25T05:57:36.93411","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["sixteenPrediction, actuals, masks = predict16(framObjTrain, unet)"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":2.547893,"end_time":"2021-09-25T05:57:45.081848","exception":false,"start_time":"2021-09-25T05:57:42.533955","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["Plotter(actuals[2], sixteenPrediction[2][:,:,0], masks[2])"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":2.550279,"end_time":"2021-09-25T05:57:50.144119","exception":false,"start_time":"2021-09-25T05:57:47.59384","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["Plotter(actuals[10], sixteenPrediction[10][:,:,0], masks[10])"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":2.518212,"end_time":"2021-09-25T05:57:54.885127","exception":false,"start_time":"2021-09-25T05:57:52.366915","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["Plotter(actuals[13], sixteenPrediction[13][:,:,0], masks[13])"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":3.017823,"end_time":"2021-09-25T05:58:00.356324","exception":false,"start_time":"2021-09-25T05:57:57.338501","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["Plotter(actuals[5], sixteenPrediction[5][:,:,0], masks[5])"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":2.546436,"end_time":"2021-09-25T05:58:05.128939","exception":false,"start_time":"2021-09-25T05:58:02.582503","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["Plotter(actuals[15], sixteenPrediction[15][:,:,0], masks[15])"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":2.673756,"end_time":"2021-09-25T05:58:10.253627","exception":false,"start_time":"2021-09-25T05:58:07.579871","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["unet.save('MapSegmentationGenerator.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","model = load_model('MapSegmentationGenerator.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","from PIL import Image\n","\n","def predict_images(model, image_paths):\n","    \"\"\"\n","    Predicts the output of the given model for a list of input images.\n","\n","    Args:\n","        model: The model used for prediction.\n","        image_paths: A list of file paths to the input images.\n","\n","    Returns:\n","        A list of predictions for each input image.\n","    \"\"\"\n","    predictions = []\n","    for path in image_paths:\n","        image = np.array(Image.open(path).resize((128, 128)))\n","        prediction = model.predict(np.expand_dims(image, axis=0))\n","        predictions.append(prediction)\n","    return predictions\n","\n","# Usage example\n","image_paths = [\n","    '/kaggle/input/test-dataset11/vit1 (1).jpg',\n","    '/kaggle/input/test-dataset11/vit2 (1).jpg',\n","    '/kaggle/input/test-dataset11/vit3 (1).jpg',\n","    '/kaggle/input/test-dataset11/vit4 (1).jpg'\n","]\n","predictions = predict_images(model, image_paths)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image\n","\n","def visualize_results(predicted_mask, original_image, save_path=None):\n","    \"\"\"\n","    Visualizes the predicted mask and the original image side by side.\n","\n","    Args:\n","        predicted_mask (numpy.ndarray): The predicted mask.\n","        original_image (numpy.ndarray): The original image.\n","        save_path (str, optional): The file path to save the visualization. If not provided, the visualization will be displayed.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    # Convert to NumPy arrays and ensure numeric data types\n","    original_image = np.array(original_image).astype(np.uint8)\n","    predicted_mask = np.array(predicted_mask).astype(np.float32)\n","\n","    # Display the original image\n","    plt.figure(figsize=(10, 5))\n","\n","    # Original Image\n","    plt.subplot(1, 2, 1)\n","    plt.imshow(original_image)\n","    plt.title('Original Image')\n","    plt.axis('off')\n","\n","    # Predicted Mask\n","    plt.subplot(1, 2, 2)\n","    plt.imshow(np.squeeze(predicted_mask), vmin=0, vmax=1)\n","    plt.title('Predicted Mask')\n","    plt.axis('off')\n","\n","    if save_path:\n","        plt.savefig(save_path)\n","    else:\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["im1 = Image.open('/kaggle/input/test-dataset11/vit1 (1).jpg')\n","im2 = Image.open('/kaggle/input/test-dataset11/vit2 (1).jpg')\n","im3 = Image.open('/kaggle/input/test-dataset11/vit3 (1).jpg')\n","im4 = Image.open('/kaggle/input/test-dataset11/vit4 (1).jpg')\n","visualize_results(predictions[0], im1)\n","visualize_results(predictions[1], im2)\n","visualize_results(predictions[2], im3)\n","visualize_results(predictions[3], im4)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# F1 scrorer\n","from sklearn.metrics import f1_score\n","def f1(y_true, y_pred):\n","    \"\"\"\n","    Computes the F1 score for the given ground truth and predicted masks.\n","\n","    Args:\n","        y_true (numpy.ndarray): The ground truth masks.\n","        y_pred (numpy.ndarray): The predicted masks.\n","\n","    Returns:\n","        The F1 score for the given ground truth and predicted masks.\n","    \"\"\"\n","    y_true = y_true.flatten()\n","    y_pred = y_pred.flatten()\n","    return f1_score(y_true, y_pred)\n","\n","# Usage example\n","y_true = np.array(Image.open('/kaggle/input/test-dataset11/vit1 (1).jpg').resize((128, 128)))\n","y_pred = predictions[0]\n","f1(y_true, y_pred)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":966140,"sourceId":1634186,"sourceType":"datasetVersion"},{"datasetId":4387440,"sourceId":7533032,"sourceType":"datasetVersion"}],"dockerImageVersionId":30302,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
